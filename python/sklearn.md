# üß† Scikit-learn - Machine Learning

## üìö O que √© o Scikit-learn?

**Scikit-learn** √© a biblioteca Python mais popular para **Machine Learning**. Oferece ferramentas simples e eficientes para an√°lise preditiva e aprendizado de m√°quina.

### üéØ **Para que serve:**
- üéì **Classifica√ß√£o** - Prever categorias (aprovado/reprovado)
- üìä **Regress√£o** - Prever valores num√©ricos (pre√ßo, temperatura)
- üéØ **Clustering** - Agrupar dados similares
- üìà **Redu√ß√£o de dimensionalidade** - Simplificar dados complexos
- üîç **Sele√ß√£o de features** - Escolher as melhores vari√°veis

## üöÄ Instala√ß√£o

```bash
pip install scikit-learn
```

## üéØ Tipos de problemas

### 1. **Classifica√ß√£o** - "Que categoria √©?"

```python
# Exemplo: Aprova√ß√£o de alunos
# Entrada: horas_estudo, faltas
# Sa√≠da: Aprovado (1) ou Reprovado (0)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
```

### 2. **Regress√£o** - "Qual √© o valor?"

```python
# Exemplo: Prever pre√ßo de casas
# Entrada: √°rea, quartos, localiza√ß√£o
# Sa√≠da: Pre√ßo (valor num√©rico)

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
```

### 3. **Clustering** - "Quais s√£o similares?"

```python
# Exemplo: Agrupar clientes por comportamento
# Entrada: idade, renda, gastos
# Sa√≠da: Grupo (1, 2, 3, ...)

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
```

## üîß Fluxo b√°sico de ML

### **1. Preparar dados**

```python
import pandas as pd
from sklearn.model_selection import train_test_split

# Carregar dados
dados = {
    'horas_estudo': [2, 4, 5, 1, 3, 6, 7, 8, 2, 5],
    'faltas': [5, 3, 2, 8, 6, 1, 0, 1, 4, 2],
    'aprovado': [0, 0, 1, 0, 0, 1, 1, 1, 0, 1]
}

df = pd.DataFrame(dados)

# Separar features e target
X = df[['horas_estudo', 'faltas']]  # Features
y = df['aprovado']                   # Target

# Dividir em treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)
```

### **2. Escolher modelo**

```python
# Para classifica√ß√£o
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Para regress√£o
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

# Para clustering
from sklearn.cluster import KMeans
```

### **3. Treinar modelo**

```python
# Criar modelo
modelo = LogisticRegression()

# Treinar
modelo.fit(X_train, y_train)
```

### **4. Fazer predi√ß√µes**

```python
# Predizer dados de teste
y_pred = modelo.predict(X_test)

# Predizer novos dados
novo_aluno = [[4, 2]]  # 4 horas, 2 faltas
resultado = modelo.predict(novo_aluno)
print("Aprovado" if resultado[0] == 1 else "Reprovado")
```

### **5. Avaliar performance**

```python
# Acur√°cia
acuracia = modelo.score(X_test, y_test)
print(f"Acur√°cia: {acuracia:.2f}")

# M√©tricas detalhadas
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
```

## üéØ Modelos principais

### **Classifica√ß√£o**

#### **1. Regress√£o Log√≠stica**
```python
from sklearn.linear_model import LogisticRegression

modelo = LogisticRegression()
modelo.fit(X_train, y_train)
```

**Vantagens:**
- ‚úÖ **Simples e r√°pido**
- ‚úÖ **Interpret√°vel** - coeficientes t√™m significado
- ‚úÖ **Bom para dados lineares**

**Limita√ß√µes:**
- ‚ùå **S√≥ funciona bem** para rela√ß√µes lineares
- ‚ùå **Pode ter overfitting** com muitas features

#### **2. √Årvore de Decis√£o**
```python
from sklearn.tree import DecisionTreeClassifier

modelo = DecisionTreeClassifier(max_depth=3)
modelo.fit(X_train, y_train)
```

**Vantagens:**
- ‚úÖ **F√°cil de entender** - regras claras
- ‚úÖ **Funciona com dados n√£o-lineares**
- ‚úÖ **N√£o precisa normalizar dados**

**Limita√ß√µes:**
- ‚ùå **Pode overfitting** facilmente
- ‚ùå **Inst√°vel** - pequenas mudan√ßas afetam muito

#### **3. Random Forest**
```python
from sklearn.ensemble import RandomForestClassifier

modelo = RandomForestClassifier(n_estimators=100)
modelo.fit(X_train, y_train)
```

**Vantagens:**
- ‚úÖ **Muito robusto** - combina v√°rias √°rvores
- ‚úÖ **Bom para dados complexos**
- ‚úÖ **Import√¢ncia de features**

**Limita√ß√µes:**
- ‚ùå **Menos interpret√°vel**
- ‚ùå **Mais lento** para treinar

### **Regress√£o**

#### **1. Regress√£o Linear**
```python
from sklearn.linear_model import LinearRegression

modelo = LinearRegression()
modelo.fit(X_train, y_train)
```

#### **2. Random Forest Regressor**
```python
from sklearn.ensemble import RandomForestRegressor

modelo = RandomForestRegressor(n_estimators=100)
modelo.fit(X_train, y_train)
```

## üìä M√©tricas de avalia√ß√£o

### **Para Classifica√ß√£o:**

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Acur√°cia - Porcentagem de acertos
acuracia = accuracy_score(y_test, y_pred)

# Precis√£o - Dos que previu positivo, quantos eram realmente?
precisao = precision_score(y_test, y_pred)

# Recall - Dos que eram realmente positivos, quantos acertou?
recall = recall_score(y_test, y_pred)

# F1-Score - M√©dia harm√¥nica entre precis√£o e recall
f1 = f1_score(y_test, y_pred)

print(f"Acur√°cia: {acuracia:.2f}")
print(f"Precis√£o: {precisao:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")
```

### **Para Regress√£o:**

```python
from sklearn.metrics import mean_squared_error, r2_score

# Erro quadr√°tico m√©dio
mse = mean_squared_error(y_test, y_pred)

# R¬≤ - Qu√£o bem o modelo explica a vari√¢ncia
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"R¬≤: {r2:.2f}")
```

## üîß Pr√©-processamento

### **1. Normaliza√ß√£o/Standardiza√ß√£o**

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# StandardScaler (z-score)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# MinMaxScaler (0-1)
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

### **2. Codifica√ß√£o de vari√°veis categ√≥ricas**

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# LabelEncoder (para target)
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# OneHotEncoder (para features)
from sklearn.compose import ColumnTransformer

categorical_features = ['cidade']
ct = ColumnTransformer(
    transformers=[('encoder', OneHotEncoder(), categorical_features)],
    remainder='passthrough'
)
X_encoded = ct.fit_transform(X)
```

### **3. Tratamento de valores nulos**

```python
from sklearn.impute import SimpleImputer

# Preencher com m√©dia
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Preencher com mediana
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)
```

## üéØ Valida√ß√£o cruzada

### **K-Fold Cross Validation**

```python
from sklearn.model_selection import cross_val_score

# 5-fold cross validation
scores = cross_val_score(modelo, X, y, cv=5)
print(f"Acur√°cia m√©dia: {scores.mean():.2f} (+/- {scores.std() * 2:.2f})")
```

### **Stratified K-Fold**

```python
from sklearn.model_selection import StratifiedKFold

# Mant√©m propor√ß√£o das classes
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(modelo, X, y, cv=skf)
```

## üîç Sele√ß√£o de features

### **1. Correla√ß√£o**

```python
import pandas as pd

# Matriz de correla√ß√£o
correlacao = df.corr()
print(correlacao['target'].sort_values(ascending=False))
```

### **2. Feature Importance (Random Forest)**

```python
from sklearn.ensemble import RandomForestClassifier

modelo = RandomForestClassifier(n_estimators=100)
modelo.fit(X_train, y_train)

# Import√¢ncia das features
importancia = modelo.feature_importances_
features = X.columns

for feature, imp in zip(features, importancia):
    print(f"{feature}: {imp:.3f}")
```

### **3. Sele√ß√£o autom√°tica**

```python
from sklearn.feature_selection import SelectKBest, f_classif

# Selecionar as 5 melhores features
selector = SelectKBest(score_func=f_classif, k=5)
X_selected = selector.fit_transform(X, y)
```

## üí° Dicas importantes

### **Boas pr√°ticas:**
- ‚úÖ **Sempre divida** em treino/teste
- ‚úÖ **Use valida√ß√£o cruzada** para avalia√ß√£o robusta
- ‚úÖ **Normalize dados** quando necess√°rio
- ‚úÖ **Teste m√∫ltiplos modelos** antes de escolher
- ‚úÖ **Documente** os par√¢metros usados

### **Comandos √∫teis:**
```python
# Informa√ß√µes do modelo
modelo.get_params()     # Par√¢metros do modelo
modelo.coef_           # Coeficientes (regress√£o linear)
modelo.intercept_      # Intercepto
modelo.feature_importances_  # Import√¢ncia (Random Forest)

# Salvando e carregando modelos
import joblib
joblib.dump(modelo, 'modelo.pkl')
modelo_carregado = joblib.load('modelo.pkl')
```

### **Pipeline completo:**
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Pipeline: normalizar + treinar
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression())
])

pipeline.fit(X_train, y_train)
acuracia = pipeline.score(X_test, y_test)
```

---

**üß† Scikit-learn √© a ferramenta essencial para Machine Learning em Python!** Oferece tudo que voc√™ precisa para criar modelos preditivos poderosos. üöÄ

---

**üìö Refer√™ncia:** [Documenta√ß√£o oficial do Scikit-learn](https://scikit-learn.org/stable/)
